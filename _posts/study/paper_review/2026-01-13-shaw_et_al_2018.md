---
layout: post
title: "Self-Attention with Relative Position Representations 논문 리뷰"
subtitle: "상대적 위치 기반 인코딩의 시작"
date: 2026-01-13 23:59:00+0900
background: '/img/bg-paper_review.jpg'
katex: true
category: Study
tags: [ paper_review ]
---

# Self-Attention with Relative Position Representations

오늘 리뷰해 볼 논문은 Self-Attention with Relative Position Representations <sup>[1](#footnote_1)</sup> 이다. 이 논문을 읽게 된 이유는, 원래 RoPE 논문을 읽고 있었는데 기습 출현했기 때문이다... 그렇지만 아이디어의 흐름을 이해하는 데 중요한 논문 중 하나인 것 같아 읽어보고 리뷰까지 진행하기로 결정했다.

## 세 줄 요약

- **위치 정보 인코딩을**

- **토큰 위치 차이 (거리) 기반으로**

- **Value 와 Key 에 더해준다.**

## 논문을 읽으며 들었던 생각의 흐름

### 논문에 익숙하지 않은 나

논문을 처음 봤을 때 든 생각은, 뭐가 이렇게 글자가 많은가 하는 생각이었다... 이 글은 최초로 논문을 리뷰하는 글인데, 아직 내가 논문을 별로 읽어 본 경험이 없어서 이런 생각이 들었을지도 모르겠다. 어쨌든 꾸역꾸역 영어들을 읽어나가다 보니, 2장에 가서야 무슨 소리를 하고 싶은지 본격적으로 이해할 수 있었다.

### 늘 보던 그 식

2.2 에서 설명하는 내용은 그냥 지겹게 보던 self-attention 수식을 다시 써놨을 뿐이었다. 한 가지 주목할 점이 있다면, $v_j$, $q_i$, 혹은 $k_j$ 같은 문자로 value, query, key 를 나타내는 대신, 꼬박꼬박 입력 $x_i$ 혹은 $x_j$ 에다가 행렬 $W^V$ 등을 곱한 표현을 이용했다는 것이다. 왜 그렇게 했을까? 그건 모르겠다. 내 뇌피셜로는, 아마 self-attention 연산이 일어나는 과정을 좀 더 명시적으로 보여주고 싶었음이 아닐까 싶었다.

### 본론

본 게임은 3장부터 시작이었다. 드디어 이 논문에서 제안하고자 하는 바를 표현하기 시작한 것이다. 논문 기준으로 (3) 식을 보면, value 값인 $x_j W^V$ 에 $a_{ij}^V$ 를 더해주는 것을 볼 수 있었다. (4) 식에서는 key 값인 $x_j W^K$ 에 $a_{ij}^K$ 를 더해주는 것을 볼 수 있었다. 처음에는 세상 온갖 첨자가 튀어나와 많이 당황스러웠는데, 생각보다 별건 아니었다. 나중에 나온 식들과 함께 보면 간단한 내용이었다.

결국 3장을 통해 저자들을 하고 싶었던 건, 토큰 간의 위치 차이 (거리) 를 기반으로 위치 인코딩을 심어주는 것이었고, 이를 위해 기존의 self-attention 식을 조금 수정했을 뿐인 것이었다. 각 $a$ 의 역할은 value / key 에 더해줄 인코딩 값을 담는 것이고, 아래 첨자에 $ij$ 라고 써있는 건 $i$ 번째 토큰과 $j$ 번째 토큰 사이의 attention 연산 과정에서 더해주는 값이라는 의미인거고, 위 첨자로 $V$, $K$ 라고 써있는 건 그냥 'V'alue 에 더하는지, 'K'ey 에 더하는 지 차이를 나타낼 뿐이었다.

3.2 에서 $a$ 들의 정체가 밝혀지는데, 처음에 식을 보고 대체 뭔소린가 한참을 고민했었다. 그러나 별 어려운 내용은 역시 아니었다. $a_{ij}^K = w_{\text{clip}(j-i, k)}^K$ 라고 써있는 것은, 위 첨자가 $K$ 니까 key 쪽에 더해주는거고, $ij$ 파트는 $j - i$ 형태로 바뀌어, 그 자체의 위치 보다는 두 위치의 차이가 반영되는데, 여기서 $\text{clip}$ 함수를 먹여서 두 개 차이가 너무 막 나가는 걸 막아준 것이다. $\text{clip}$ 함수가 무엇인가 하니, 양수 뱡향이던 음수 방향이던 절댓값이 주어진 어떤 $k$ 를 벗어나면 강제로 그 절댓값을 $k$ 로 제한하는 것이었다. 예를 들어, $k = 10$ 이라고 하면, 두 토큰 위치 차이가 $6$ 이면 $\text{clip}(6, 10) = 6$ 인 것이고, 두 토큰 위치 차이가 $-20$ 이면 $\text{clip}(-20, 10) = -10$ 인 것이다.

그래서 그 더해주는 값들의 종류는 key 와 value 쪽 각각 $w_{-k}$ 부터 $w_k$ 까지 총 $2k+1$ 종류의 값들이 필요하고, 논문에서는 이것들을 학습시키면 된다고 한다. 사실 이 시점에서 논문에서 하고 싶은 말은 거의 다 끝났다.

뒷 부분은 효율적인 구현을 위해 식을 다르게 전개해서 연산을 진행하는 내용과, 실험 결과를 담고 있는데, 솔직히 논문의 아이디어와 방법론을 이해하는데 별로 도움이 되지는 않으니 설명을 스킵하겠다.

### 주요 의문점

논문을 다 읽고 나니 한 가지 궁금한 부분이 생겼다. 참고로 이 의문에 대한 명쾌한 답은 찾지 못했다.

> 어째서 위치 인코딩을 key 와 value 쪽에만 심어두고 query 는 가만히 내버려두었을까?

내 뇌피셜에 따르면, 연구진들은 딱히 query 만 차별하려고 했던 것은 아닐 것 같다. 원래대로라면 query, key, value 에 전부 넣으려고 했겠지만, '어짜피 query 와 key 는 곱해지는데, 둘 중 한 곳에만 위치 정보를 넣는 것으로 충분하지 않나?' 라는 식의 아이디어가 아니었을까 싶다.

그렇다면 query 와 key 둘 중 하나를 고르는데 어째서 그것이 key 여야 했는가에 대한 의문은, 나름대로 2가지 가설이 있다.

1. 첫 번째로는 key 쪽이 value 와 같은 $x$ 에 대한 인덱스를 가지고 있기 때문이고,

2. 두 번째로는 query 와 key 에 대한 의미론적인 느낌 상, query 는 광장에다가 대고 '누구 나랑 관련있는 사람 없습니까?' 하고 소리지르는 느낌이고, key 는 '나 여깄소!' 하고 대답하는 느낌이니까, 대답하는 쪽이 상대적 위치 정보를 들고 있는게 더 자연스럽다고 생각했을 것이기 때문이다.

당연히 둘 다 내 뇌피셜이고, 논문에서는 관련 설명을 발견하지 못했다. 이 의문은 다른 논문들을 읽다보면 저절로 풀리게 될까? 그건 시간만이 말해줄 것이다.

## 마무리

최초로 '논문 리뷰' 라는 걸 해봤다. 사실 논문 리뷰라고는 해도, 읽으면서 들었던 생각이나 의문점을 정리해보는 거라 일반적인 느낌의 논문 리뷰와는 거리가 있지만, 어짜피 내용을 친절하게 상세하게 잘 풀어둔 글들은 세상에 많기 때문에, 내가 그런 중복되는 중노동을 해봐야 ~~아무도 안 읽기 때문에~~ 의미가 없다고 생각했다.

이런 식으로 다른 논문들도 읽어보고 가능하면 글로 정리해볼 계획이다. 논문 리뷰가 고작 이 논문 하나로 끝나지 않았으면 좋겠다.

오늘은 여기까지!

---
<a name="footnote_1">1</a>: <https://arxiv.org/abs/1803.02155>  